\section*{Introduction}\label{sec:introduction}

In recent years, companies such as Google, AWS, IonQ, IBM, and Microsoft have led the race to build a practical quantum computer (QPU). Up to this day, a number of chips have been released, including Willow \cite{acharyaQuantumErrorCorrection2025_2025}, Majorana 1 \cite{aasenRoadmapFaultTolerant2025_2025}, Ocelot \cite{puttermanHardwareefficientQuantumErrora_2025} and Heron \cite{abughanemIBMQuantumComputers_2025}, taking quantum utility one step closer. A common characteristic of these advanced chips is the enhancement in error correction techniques. In particular, the chips developed by Google and Amazon employ randomized benchmarking (RB) techniques to estimate average error rates. The focus on error correction is fundamental to fully exploiting the potential of logical qubits, which represent an advanced abstraction level wherein information is shielded from noise by being encoded across multiple physical qubits.

The logical qubit constitutes the most advanced solution to the substantial error rates inherent in quantum chips \cite{acharyaSuppressingQuantumErrors_2023}. Rather than centralizing the information in a singular qubit, this approach distributes information across various nodes of physical qubits. Such nodes are governed by a quantum error correction algorithm that dynamically detects and rectifies errors as they occur, thereby improving the stability of the computation and ensuring reliable results.

Although the logical qubit offers a powerful solution for error mitigation, its implementation and effectiveness vary between different hardware. This variability makes it increasingly important to adopt a set of standardized metrics that allow companies to compare the performance of their state-of-the-art QPUs. One recent metric includes IBM's "layer of fidelity", which evaluates the fidelity of two-qubit gates connected over N qubits, allowing developers to estimate how many circuits are required for error mitigation \cite{mckayBenchmarkingQuantumProcessor2023_2023}. Therefore, one of the challenges associated with the quantum utility era is defining a set of valuable metrics to evaluate QPUs. Furthermore, finding metrics to validate quantum software in distributed systems seems a natural next step in benchmarking.

The task of benchmarking a QPU represents a particularly complex aspect of identifying valuable metrics, as it demands the evaluation of a technology that is constructed upon multiple layers of abstraction. As of today, logical qubits are distinguished from physical qubits, and benchmarking techniques are used to perform tests on the logical qubit layer. There are two reasons for this: 1) physical qubits are not accessible directly, and 2) the logical qubit architecture includes algorithms and calibration protocols to deal with errors caused by noise and other external factors \cite{campbellRoadsFaulttolerantUniversal2017_2017, tomitaLowdistanceSurfaceCodes_2014}. Consequently, QPU benchmarking has traditionally involved calculating correlations between execution results and pre-runtime metrics such as quantum volume, quantum number, circuit depth, and average error gate (calculated with RB) \cite{proctorBenchmarkingQuantumComputers2025_2025}.

In the literature, multiple benchmarking techniques for a single QPU are described. One common technique is Randomized Benchmarking (RB). RB works by implementing sequences of randomly sampled quantum gates (typically from the Clifford group) and measuring how the fidelity of the final quantum state decreases as the sequence length increases \cite{emersonScalableNoiseEstimation_2005}. As a result, the failure rate is directly correlated with the average fidelity of the gate, which helps calculate the measurement error \cite{magesanRobustRandomizedBenchmarking_2011}.

Other QC benchmarks include high-level holistic approaches such as quantum volume, noisy intermediate-scale quantum algorithms, quantum error correction algorithms, computational problems, and many-qubit standards; low-level holistic methods including mirror circuits, algorithmic benchmarks, direct RB, and cross-entropy; and low-level component benchmarking through tomographic methods and cycle benchmarking \cite{proctorBenchmarkingQuantumComputers2025_2025}.

Although the benchmarking techniques described above provide valuable insight, they are currently limited to evaluating isolated systems, such as single QPUs. As quantum computing, like classical computing, naturally progresses toward distributed architectures, it becomes essential to develop benchmarks that can be executed across physically separated quantum processors. \cite{caleffiDistributedQuantumComputing_2024}

In classical computing, the performance of a distributed system is measured through throughput and latency, its scalability is tested by examining the workload on the client and server, the availability of the system is often measured by the degradation of throughput caused by failure, and the consistency of the system is often measured by the staleness of data across different locations \cite{andreoliniBenchmarkingModelsTools_2002}. Unlike classical computing, QC's distributed system is just getting started. As a first step towards distributed QC, researchers have already achieved teleportation between two physically separate QPUs \cite{kangTeleportingTwoqubitEntanglementa_2025}. 

Motivated by this progress and the broader need for distributed benchmarking, we introduce an algorithm inspired by the teleportation protocol (TP) that can benchmark either a single or multiple QPUs. As a high-level overview, the algorithm is designed to scale in qubits by connecting bell pairs, and incorporates a RB benchmark applied to both the payload and transport state to later calculate post-runtime metrics. This approach provides a framework for investigating the fundamental aspects of quantum circuits. Specifically, our work is driven by the following research question.
\begin{enumerate}[label=RQ\arabic*,ref=RQ\arabic*]
    \item \label{q:RQ1} How do the pre-runtime structural characteristics of quantum circuits (such as payload size, circuit depth, gate count, and gate types) relate to post-runtime outcomes, including execution success rates, execution time, cost, and quantum information degradation?
    % \item \label{q:RQ2} How can pre-runtime metrics be utilized—such as through embedding vectors—to enable the prediction of post-runtime metrics for arbitrary quantum circuits?
\end{enumerate}

The remainder of this paper is organized as follows. Section \nameref{sec:background} provides context on randomized benchmarking and TP. Section \nameref{sec:algorithm} introduces vTP and develops its theoretical properties using the Dirac notation. Section \nameref{sec:experiment} presents our observations and results. Finally, Section \nameref{sec:conclusions} summarizes our findings.
