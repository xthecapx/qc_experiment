\section{Introduction}\label{sec:introduction}

\PARstart{I}{n} recent years, companies such as Google, AWS, IonQ, IBM, and Microsoft have led the race to build a practical quantum computer (QPU). Up to this day, a number of chips have been released, including Willow \cite{acharyaQuantumErrorCorrection2025_2025}, Majorana 1 \cite{aasenRoadmapFaultTolerant2025_2025}, Ocelot \cite{puttermanHardwareefficientQuantumErrora_2025} and Heron \cite{abughanemIBMQuantumComputers_2025}, taking quantum utility one step closer to practical realization. A common characteristic of these advanced chips is the enhancement in error correction techniques. In particular, the chips developed by Google and Amazon employ randomized benchmarking (RB) techniques to estimate average error rates. The focus on error correction is fundamental to fully exploiting the potential of logical qubits, which represent an advanced abstraction level wherein information is shielded from noise by being encoded across multiple physical qubits.

The accelerated advancement in hardware technology presents a significant challenge for software developers who use this technology. With powerful QPUs becoming more accessible, the focus of the community shifts from the feasibility of hardware construction to the reliability of circuit execution, a challenge that defines the core of our research.

To ensure reliable results, developers currently rely on a combination of logical qubits and benchmarking protocols, each with specific limitations from a circuit-reliability prediction standpoint. For instance, logical qubit abstraction aims to mitigate errors through foundational correction codes \cite{acharyaSuppressingQuantumErrors_2023}, but its hardware-dependent implementation makes it difficult for a developer to anticipate how their unique circuit structure will interact with the error correction layer. Similarly, hardware-centric benchmarks like Quantum Volume \cite{proctorBenchmarkingQuantumComputers2025_2025} or IBM's "layer of fidelity" \cite{mckayBenchmarkingQuantumProcessor2023_2023} provide a single holistic score based on random circuits, which reveals little about how a specific structured algorithm will behave. Even Randomized Benchmarking (RB), while excellent for calculating an average gate error rate \cite{emersonScalableNoiseEstimation_2005, magesanRobustRandomizedBenchmarking_2011}, is explicitly designed to eliminate sequence-specific errors that are critical to the reliability of a particular application. This failure to deliver circuit-specific predictive insights makes these existing approaches unsuitable for improving the understandability of circuits, nor can they serve as a foundational framework for the development of reliability models for user-defined circuits.

This disconnect has led to calls within the research community for more application-motivated benchmarks that can bridge the gap between general hardware capability and specific circuit reliability \cite{millsApplicationMotivatedHolistic2021}, revealing a knowledge gap in the quantum computing domain. Addressing this gap requires moving beyond a purely hardware-focused evaluation. The task involves a technology built on multiple layers of abstraction, from inaccessible physical qubits to the error-corrected logical qubit layer, where tests are performed \cite{campbellRoadsFaulttolerantUniversal2017_2017, tomitaLowdistanceSurfaceCodes_2014}.

To bridge this gap between general hardware capability and specific circuit reliability, this paper introduces a comprehensive approach for circuit evaluation. We propose a complete benchmarking strategy that includes a methodology for generating controlled random circuits to systematically validate the correctness of the circuit. In our experimentation, we establish a crucial correlation between pre-runtime metrics (such as number of gates and two-qubit gate count) and post-runtime outcomes (e.g., success rate), providing a predictive link between a circuit's design and its execution fidelity.

The heart of our approach is the novel algorithm we term the variation of the Teleportation Protocol (vTP). The protocol is named because it utilizes the same initial entanglement structure as the canonical teleportation protocol. But, instead of using the measurement operations and the required classical communication, we propose the use of fixed local unitary operations to reconstruct the state of the \emph{payload}, i.e., the set of qubits and gate operations that constitute the quantum state being benchmarked. This modification transforms the teleportation protocol from a communication utility into a testing environment that can be used on a single QPU today while being designed for future distributed quantum services.

This framework allows us to investigate the fundamental aspects of circuit reliability. Specifically, our work is driven by the following research question.

\begin{enumerate}[label=RQ\arabic*,ref=RQ\arabic*]
\item \label{q:RQ1} How do the pre-runtime structural characteristics of quantum circuits (such as payload size, circuit depth, gate count, and gate types) relate to post-runtime outcomes, including execution success rates, execution time, cost, and quantum information degradation?
\end{enumerate}

The remainder of this paper is organized as follows. Section \ref{sec:background} presents a review of the classification of benchmarking techniques, including our formulated benchmarking approach, and describes both pre-runtime and post-runtime metrics. Sections \ref{sec:tp} and \ref{sec:vtp} outline the mathematical formulation and conduct simulations for both the teleportation protocol and a variant. Section \ref{sec:experiment} presents our observations and results. Finally, Section \ref{sec:conclusions} summarizes our findings.