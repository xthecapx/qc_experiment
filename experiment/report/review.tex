to reference: \cite{malhotraSystematicReviewQuantum_2024}

Benchmarking variants 

Randomized benchmarking 
Application-oriented benchmarking 
Volumetric benchmarking
Quantum tomography Miscellaneous


2.3 Benefits of Randomized Benchmarking 

Randomized benchmarking (RB) was the most widely used approach observed in the selected research papers because it is a robust and straightforward method for evaluating quantum gate performance. By averaging error rates over random sequences of quantum operations, RB effectively identifies and mitigates systematic errors that might not be detected through other techniques. Additionally, RB is less affected by state preparation and measurement errors, making it a reliable and widely applicable method for assessing the performance of various quantum systems and platforms.


2.4 Benefits of Application-oriented Benchmarking Application-oriented benchmarking helps us see how well quantum systems handle realworld tasks by running quantum algorithms and protocols. It shows us where these systems work well and where they need improvement, giving us practical insights. By focusing on specific applications, we can identify the strengths and weaknesses of quantum technology, which helps us improve and optimize it. 3 Preliminaries Quantum benchmarking is essential for determining the performance and reliability of quantum computers. As quantum computing technology advances, the need to accurately measure and compare the capabilities of different quantum processors becomes increasingly important. This section outlines why quantum benchmarking is necessary, followed by a detailed discussion of key performance metrics and an overview of various benchmarking methods. The reader can refer to various comprehensive surveys in the area of quantum computing, specifically, quantum walk [10], quantum programming language [11], quantum software testing [12], non-classical automata [13], Quantum Software [14] and quantum cryptography [15], for in-depth understanding the basics of quantum computing. 3.1 The Need for Quantum Benchmarking Current quantum computers are significantly affected by noise, which degrades their performance and reliability. Noise sources include decoherence, operational errors, and environmental disturbances. Benchmarking is essential to identify these potential noise sources, as it characterizes the types and magnitudes of errors, helping to pinpoint specific noise sources that affect quantum operations. Additionally, benchmarking is crucial for tracking progress in the performance of quantum computers, allowing for the comparison of improvements over time and across different quantum processors and technologies. Regular benchmarking results provide critical feedback for developing error correction techniques and noise mitigation strategies, driving quantum computing hardware and algorithm advances (Figs. 2,3,4,5, 6,7,8,and 9). By offering insights into quantum processors’ practical utility and limitations, benchmarking guides the development and optimization of more robust and reliable quantum computing technologies.


3.2 Key Metrics in Quantum Benchmarking Several key metrics are used to evaluate the performance of quantum computers effectively. These metrics provide a comprehensive picture of a quantum processor’s quality and operational efficacy, enabling researchers to identify strengths and weaknesses and to make informed improvements. The following are commonly used key metrics in quantum benchmarking: 1. Qubit Count refers to the number of qubits of a quantum processor. A higher qubit count generally indicates a more powerful quantum computer in handling more complex computations. 2. Relaxation Time (T1) and Dephasing Time (T2) measure the coherence properties of a qubit. Relaxation time is when a qubit loses its state due to energy loss, whereas dephasing time measures the time that a qubit maintains its phase coherence. Longer relaxation and dephasing times are indicative of better qubit stability and performance. 3. Qubit Connectivity describes how qubits in a quantum processor are connected. Better connectivity allows for more efficient quantum gate operations and algorithms, reducing the overall error rates in computations.
4. Fidelities measure how accurately quantum gates and operations perform their intended tasks. High fidelities indicate that the gates and operations are very close to their ideal performance, which is crucial for reliable quantum computations. Fidelity is measured using 5. Gate Errors refer to the deviations from perfect gate operations. These errors can arise from various sources, such as decoherence, control errors, and environmental noise. Lower error rates are indicative of higher performance and reliability in quantum processors. 6. Quantum Volume, developed by IBM, is a comprehensive metric that evaluates the performance of a quantum processor taking into account multiple factors. The method looks at the number of qubits, the quality of the gates, how long the qubits stay stable (coherence times), and how well the qubits are connected. This metric produces a single score that indicates the overall capability of the processor. It measures the largest complex circuit-with an equal number of qubits and gate layers-that the processor can execute reliably. 3.3 Types of Benchmarking Quantum Benchmarking is classified into the following types: 1. Randomized Benchmarking is a fundamental protocol in quantum computing used to assess the error rates of quantum gates, particularly those from the Clifford group. The protocol involves randomly selecting sequences of Clifford gates, applying them to the quantum system, and measuring the fidelity of the resulting state compared to the ideal state. The key idea behind randomized benchmarking is to use sequences of gates that are "random" enough to probe the average behavior of the quantum system while avoiding biases introduced by specific gate sequences. 2. Application-oriented Benchmarking is a specialized technique designed explicitly for evaluating quantum computers. Unlike typical benchmarks that focus solely on hardware performance, application-oriented benchmarking assesses the practical usability of quantum computing systems by simulating real-world scenarios and tasks to measure the quantum computer’s effectiveness in solving complex problems. 3. Volumetric Benchmarking involves the use of rectangular circuits of width w (number of qubits) and depth d (number of gate layers) to understand the performance metrics of space versus time of a quantum computer. 4. Tomography refers to techniques used to reconstruct a system’s internal structure or properties through measurements taken from different perspectives. In the context of quantum computing, tomography encompasses several methods, including state tomography, process tomography, and gate set tomography. State tomography reconstructs unknown quantum states by measuring observables on identically prepared quantum systems. In contrast, process tomography characterizes unknown quantum operations by preparing input states, applying operations, and measuring the resulting output states. Gate set tomography systematically probes the quantum processor with various input states and gate sequences to understand individual gate behavior and interactions. 5. Cycle Benchmarking is a method used in quantum computing to diagnose and understand errors of quantum systems in a comprehensive way. It involves repeatedly cyclically applying sequences of quantum gates to amplify specific errors present in the hardware. The process begins with preparing an initial quantum state, repeatedly using a sequence of quantum gates, and then measuring the resulting state after several cycles. It allows for identifying and quantifying errors such as cross-talk, error correlations, and errors on idling qubits.

4.2 Randomized Benchmarking Knill et al. [39] utilized randomized benchmarking on a 9Be+ trapped ion quantum computer to determine the average fidelity impacted by gate noise. They created four sets of random gate sequences and then truncated each to 17 different lengths. These sequences were Pauli-randomized eight times, resulting in 544 unique sequences. The findings revealed an exponential decay that was analysed to establish an average probability of error per gate of 0.00482. Ryan et al. [40] implemented the randomized benchmarking protocol introduced by E. Knill on a liquid-state nuclear magnetic resonance quantum processor and obtained an average error rate of 1.3 ± 0.1 × 10−4 for a single qubit processor and an average error rate of 4.7 ± 0.3 × 10−3 for one- and two-qubit gates on a three-qubit processor. Magesan et al. [41] commissioned the development of a new approach based on randomly selecting a Clifford sequence followed by an inversion operator such that the product of the entire sequence amounts to the identity. They used these sequences to determine the average sequence fidelity with respect to two realistic noise models. Gambetta et al. [42] implemented a similar method to quantify crosstalk and other unwanted interactions between coupled qubits to calculate the addressability present in the system. Gaebler et al. [43] introduced their protocol to benchmark multiqubit gates and experimentally measure the performance for two-qubit Clifford unitaries. They obtained an average error per two-qubit Clifford Unitary of 0.162 ± 0.008 and an error per phase gate of 0.069±0.017. Corcoles et al. [44] realized randomized benchmarking for two superconducting qubits with specific attention to their classicalization of the Clifford group carried out in order to decrease the usage of two-qubit gates. In this way, the fidelity of averaged error per Clifford gate came out to be 0.0936. They further performed an experiment in which they repeatedly switched between their optimized gate and a random two-qubit Clifford gate achieving error per two-qubit gate of 0.0653. Randomized benchmarking in these studies provided lower error estimates compared to tomography-based methods, which fail to account for state preparation and measurement (SPAM) errors, thus offering greater reliability. Wallman and Flammia [45] confirmed the reliability of randomized benchmarking for characterizing timedependent Markovian noise. Epstein et al. [46] analyzed the performance of benchmarking protocols under various error models, including systematic rotations, damping, leakage, and 1/f noise, finding that benchmarking provided more accurate average error estimates-often better by a factor of two. Barends et al. [47] used randomized benchmarking on a five-qubit superconducting processor with nearest-neighbor coupling to optimize one- and two-qubit gate fidelities, demonstrating that Josephson quantum computing is a high-fidelity technology suitable for large-scale fault-tolerant circuits. Kelly et al. [48] explored quantum error correction (QEC), focusing on preserving classical states amidst environmental errors. Using a nine-qubit linear array similar to the surface code QEC scheme, they track errors through repeated projective quantum non-demolition (QND) parity measurements. They used randomized benchmarking to quantify the two-qubit gate fidelity for effective error mitigation. Results show significant error suppression, with notable reduction factors of 2.7 for five qubits and 8.5 for nine qubits after eight cycles. Muhonen et al. [49] implemented the SRB and IRB protocols on the electron and nuclear spins of 31P atoms in silicon. The results showed the promising nature of donor-based quantum computers using silicon with gate fidelity of 99.95\% for the electron spin and 99.99\% for nuclear spin. Cross et al. [50]introduced a scalable protocol for non-Clifford gates, allowing a complete characterization of all the gates required in popular quantum computing protocols. Rafael et al. [51] expanded the Randomized Benchmarking protocol to include Measurement-Based Quantum Computation (MBQC) on linear cluster states. They introduced two methods for estimating the average fidelity of single-qubit gate sets: one based on the single-qubit Clifford group and another utilizing measurement-based unitaries. The second approach leverages the inherent randomness of MBQC to implement random sequences of gates efficiently. Sheldon et al. [52] introduced the iterative randomized benchmarking protocol, which utilizes an interleaved target gate set to distinguish between unitary and non-unitary errors. Using this data, they could calibrate for the unitary errors and obtain a single-qubit gate fidelity of 99.95\%. Chasseur et al. [53] introduced a new hybrid benchmarking protocol that utilizes Interleaved Randomized Benchmarking with Monte Carlo sampling of quantum states, which they proved performs better than standard Monte Carlo sampling of average fidelity due to the exponential saving in classical computation. Proctor et al. [54] revealed discrepancies between traditional Randomized Benchmarking (RB) assumptions and actual error metrics. Their work challenges the idea that mean average gate infidelity (AGI) reliably 
corresponds to the error metric “r”. They introduced new theories of RB decay for errors described by process matrices, emphasizing a simple exponential decay pattern. Wootton [55] benchmarked the IBM and Rigetti devices in the 5-19 qubit range by utilizing random circuits to accumulate performance data and analysing them to present quantitative results on their performance. Brown and Eastin [56] showed that utilizing unitary 2-design, though it is a convention for randomized benchmarking, is not a necessary condition for creating protocols utilizing subgroups of the Clifford group. Their findings showed error probability estimation within a factor of two or less, with exponential decay in qubit number. Hashagen et al. [57] proposed a real randomized benchmarking method that delivers accurate estimates of average error rates in both the real and complex components of a quantum channel. Their technique provided more granular insights into error rates while maintaining costs comparable to traditional benchmarking protocols. The method relies on the real Clifford group and a subgroup of the complex Clifford group, forming an orthogonal 2-design. It also facilitates the benchmarking of fault-tolerant gates for particular quantum encodings. Proctor et al. [58] proposed a new benchmarking method for multi-qubit quantum processors, addressing limitations of the standard Clifford randomized benchmarking. Their approach, demonstrated on 2 to 5 qubits with IBMQX5, avoids complex gate compilation by using random circuits over native gates. Qi and Ng [59] in their paper corroborated the findings of Proctor et al. [54] regarding the standard randomized benchmarking (RB) protocol. They confirmed that the results obtained from the RB protocol (r) are not equal to the average gate-set infidelity (‘’). Contrary to previous theoretical expectations, their supports the notion that RB does not directly measure the average infidelity. Wright et al. [19] investigated the accuracy of single-qubit operations in an 11-qubit quantum processor using randomized benchmarking. They introduced randomized sequences of π/2 gates and then analyzed how closely the actual results matched the predicted states. Their method allowed them to measure the gate’s performance, revealing an average single-qubit fidelity of 99.5\%, along with a 99.3\% fidelity for state preparation and measurement (SPAM) across the system. Helsen et al. [60] introduce character randomized benchmarking, a method to estimate the average fidelity of quantum gatesets, including non-Clifford groups. Their approach was based on representation theory and requires minimal experimental overhead. They demonstrated its efficacy with a gate set including the T-gate and proposed an interleaved benchmarking protocol for two-qubit Clifford gates using single-qubit Clifford gates as a reference. Harper et al. [61] showed that though randomized benchmarking and its variants are widely used for characterization, they allocate their resources sub-optimally. To resolve this issue, they modified the standard protocol, improving efficiency without compromising precision. Boone et al. [62] examined the variations between two primary standards for randomized benchmarking: the original protocol based on the Clifford group and an alternative introduced by the NIST group. Their analysis demonstrated notably different exponential decay rates in fidelity, resulting in a threefold disparity in estimated error rates under practical conditions. McKay et al. [63] performed three-qubit randomized benchmarking on a quantum device featuring three fixed-frequency transmon qubits in a pioneering study. They measured configurations with all-to-all and linear gate connectivity, emphasizing the correlations between errors in three qubits and those in one or two qubits. Additionally, Jafarzadeh et al. [64] presented unitary-gate randomized benchmarking (URB) specifically for qudit gates, extending the concept of single- and multi-qubit URB to accommodate single- and multi-qudit gates. They established a qudit URB procedure that employs unitary 2-designs, which is distinct from the approach used in the multi-qubit scenario. Derbyshire et al. [65] proposed a randomized benchmarking-based protocol for an analogue setting to benchmark Analogue quantum simulators and can deal with SPAM errors. Analogue Randomized Benchmarking measures the average error rate per time evolution of a family of Hamiltonians, and the experimental data obtained for different noise models show good agreement with theoretical predictions. In the thesis, Boone [66] approached the problem of quantum errors in large-scale systems and emphasized Randomized Benchmarking protocols similar to the devices to deal with the state preparation and measurement errors. Their first attempt aimed to understand the meaning of unitarity, introducing an experimental realization of such a user-operation feature, which was simulative due to gate-dependent noise. In a subsequent study, they offered numerical and analytical evidence supporting the correlation between the decay parameter (p) in Standard Randomized Benchmarking (SRB) and the decay parameter associated with gate-set circuit fidelity. They identified a basis mismatch as the primary reason for the discrepancies between average gate fidelity and SRB outcomes in high-fidelity single-qubit experiments. The third section of the thesis compared the results of two variations of the SRB protocol-uniform sampling (SRB) and non-uniform sampling (NIST RB) derived from the Clifford group-demonstrating that NIST RB produces an exponential fidelity curve with differing infidelity results compared to SRB. Finally, they established a standard for evaluating basic gate sets, presenting experimental findings from assessments conducted on superconducting and ion-trap devices. They also illustrated how systematic error bounds can be improved by considering error coherence and utilizing weaker randomizing sets. Liu et al. [67] proposed a noise benchmarking algorithm based on random circuit sampling, which effectively captures total noise within a globally and arbitrarily correlated noise model by measuring linear cross-entropy. In contrast to cycle benchmarking, RCS benchmarking is suitable for non-Clifford gates, thereby enabling the benchmarking of general quantum circuits. Proctor et al. [68] introduced a scalable method for randomized benchmarking utilizing mirror randomized circuits consisting of Clifford gates. They demonstrated the scalability of their approach on 16 qubits of an IBMQ processor and used simulations for up to 225 qubits. Helsen et al. [69] developed a framework for randomized benchmarking to efficiently evaluate the quality of quantum gates. They addressed previous limitations in error models and gate sets and formulated conditions for accurately describing randomized benchmarking experiments. They introduced modern signal-processing techniques to analyze data and proposed post-processing techniques to improve efficiency. Chen et al. [70] introduced the universal randomized benchmarking (URB) framework, which eliminates the need for a group structure and presents a general "post-processing" POVM. They focus on twirling schemes within URB, where the probability of measurement as a function of gate length exhibits single exponential decay if the twirling map closely matches the Haar twirling map. Their analysis relies on the matrix perturbation theory of linear operators on quantum channels. Ohkura et al. [71] performance degradation in quantum multiprogramming on NISQ processors due to crosstalk. They also proposed a low-cost, software-based crosstalk detection protocol using randomized benchmarking. Their results highlight a tradeoff between success rate and execution time, offering valuable insights for optimizing processor throughput and job management in high-demand scenarios. Helsen et al. [72] introduced matchgate benchmarking as a method for effectively determining the fidelity of numerous qubit quantum circuits through the use of continuously parametrized two-qubit gates. They integrated randomized benchmarking methods with the theoretical framework of matchgate circuit representation. Furthermore, they evaluated the efficacy of matchgate circuits created from two-qubit XY spin interactions on a Rigetti Aspen-8 quantum processor. McKay et al. [73] introduced “layer fidelity," a benchmark for assessing quantum processors at scale focusing on measuring the fidelity of two-qubit gates over multiple qubits. Their results on IBM processors offered crosstalk awareness and aligned well with mirror randomized benchmarking for many error models. Shaffer et al. [74] applied continuously-parameterized quantum gates on small quantum processors. They developed a sample-efficient procedure using random sequences of native gates and their approximate inverses, ensuring the system returns near its initial state. Their method provided lower variance in fidelity estimates compared to cross-entropy benchmarking. Furthermore, they demonstrated this technique on Sandia QSCOUT’s trapped-ion processor and IBM Q’s superconducting processor, confirming its numerical and experimental effectiveness. Mayer et al. [75] demonstrated that mirror benchmarking results in an exponential decrease in survival probability as sequence length increases under uniform noise, providing a novel approach to estimating noise coherence. They conducted mirror benchmarking experiment on the Honeywell System Model H1. Jamadagni et al. [76] developed a containerized toolchain to perform various types of benchmarking on a local High Performance Computing (HPC) cluster. They recreated the Random Quantum Circuits (RQC) sampling experiment implemented on the Sycamore chip. The reviewed studies show that randomized benchmarking (RB) is frequently employed to assess the average error rates of quantum gates by utilizing sequences of randomly selected gates. However, randomized benchmarking encounters scalability limitations, requiring O(n2/ log n) two-qubit gates to realize an n-qubit Clifford gate. Traditional RB has mostly been applied to systems with up to 3 qubits, but Proctor’s work successfully extended this to 5 qubits using Direct RB. Furthermore, recent papers have introduced mirror-randomized benchmarking as a more scalable and effective technique, which enhances the reliability of benchmarking in larger quantum systems. Table 4shows the summary of randomized benchmarking along with the processors used, fidelity and error encounter.

5 Conclusion The concept of Quantum computing was first introduced in 1980 and holds promise across domains such as cryptography, optimization, and material science. Quantum benchmarking plays an important role in ensuring the rapid development of this technology and, consequently, the construction of fault-tolerant quantum machines. Throughout this review, we discuss various quantum benchmarking techniques, providing detailed insights into their applications and methodologies. Our analysis dives deep into the research conducted by researchers in quantum benchmarking and its variants. This compilation represents our effort to consolidate research across various benchmarking techniques to the best of the author’s knowledge.

